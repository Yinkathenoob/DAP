{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "740d0831-d810-4985-86d2-a1efabbf669e",
   "metadata": {},
   "source": [
    "![Binary code with a magnifying glass](binary.jpg)\n",
    "\n",
    "As a data engineer, you often face unexpected challenges in workflows. In this scenario, the `load_and_check()` function, in charge of managing sales data, encounters issues after the latest update. Unfortunately, your colleague who usually handles this code is currently on holiday, leaving you to troubleshoot.\n",
    "\n",
    "Your task is to identify and address the issues in the sales data pipeline **without getting into every line of code.** The `load_and_check()` function loads the `sales.csv` dataset and performs several checks. Initially, it verifies the dataset's shape, ensuring it matches expectations. Subsequently, integrity checks are conducted to maintain data consistency and flag any anomalies.\n",
    "\n",
    "The `sales.csv` dataset has various columns, focusing on critical fields such as `Total`, `Quantity`, `Unit price`, `Tax`, and `Date`. It's essential that the `Tax` column accurately represents 5% of the subtotal, calculated from the `Unit Price` multiplied by `Quantity`.\n",
    "\n",
    "**Your goal is to sort out the pipeline issues, aiming for the code to return 2 success messages upon completion.** Make sure the input data remains accurate. Any necessary adjustments should be made inside the function. Do not manually edit the CSV file or pre-process the data outside of the function. Be mindful of updating any relevant if statements in the checks as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88376b5",
   "metadata": {},
   "source": [
    "#### Run the load_and_check() function first. Look for any error messages in the output - these will point you to what needs fixing. You should aim for exactly two success messages: \"Data loaded successfully\" and \"Data integrity check was successful!\"\n",
    "\n",
    "#### Review the load_and_check() function code, paying special attention to the two integrity checks:\n",
    "\n",
    "#### The first check validates the column count.\n",
    "\n",
    "#### The second check validates data integrity based on Condition_1 (Total values) and Condition_2 (Tax calculations at 5%)\n",
    "\n",
    "#### Your task is to identify and fix issues within the load_and_check() function itself, not by modifying the raw sales.csv dataset. You are permitted and expected to correct any columns within the function. Ensure the function only returns two success messages when completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64619d2f-1d45-418d-8ec6-37fd5db9a4c5",
   "metadata": {
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 59,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1738311374632,
    "lastExecutedByKernel": "4795fddd-cfd4-4aca-873a-e619cbac24be",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import pandas as pd\n\ndef load_and_check():\n    # Step 1: Load the data and check if it has the expected shape\n    data = pd.read_csv('sales.csv')  \n    \n    expected_columns = 18\n    actual_columns = data.shape[1]\n    if actual_columns != expected_columns:\n        print(f\"Data column mismatch! Expected {expected_columns}, but got {actual_columns}.\")\n        print(f\"Columns found: {list(data.columns)}\")\n    else:\n        print(\"Data loaded successfully.\")\n\n    # Step 2: Calculate statistical values and merge with the original data\n    grouped_data = data.groupby(['Date'])['Total'].agg(['mean', 'std'])\n    grouped_data['threshold'] = 3 * grouped_data['std']\n    grouped_data['max'] = grouped_data['mean'] + grouped_data.threshold\n    grouped_data['min'] = grouped_data[['mean', 'threshold']].apply(lambda row: max(0, row['mean'] - row['threshold']), axis=1)\n    data = pd.merge(data, grouped_data, on='Date', how='left')\n\n    # Condition_1 checks if 'Total' is within the acceptable range (min to max) for each date\n    data['Condition_1'] = (data['Total'] >= data['min']) & (data['Total'] <= data['max'])\n    data['Condition_1'].fillna(False, inplace=True)  \n\n    # Condition_2 checks if the 'Tax' column is properly calculated as 5% of (Quantity * Unit price)\n    data['Condition_2'] = round(data['Quantity'] * data['Unit price'] * 0.05, 1) == round(data['Tax'], 1)\n        \n    # Step 3: Check if all rows pass both Condition_1 and Condition_2\n    # Success indicates data integrity; failure suggests potential issues.\n    failed_condition_1 = data[~data['Condition_1']]\n    failed_condition_2 = data[~data['Condition_2']]\n\n    if failed_condition_1.shape[0] > 0 or failed_condition_2.shape[0] > 0:\n        print(f\"Data integrity check failed! {failed_condition_1.shape[0]} rows failed Condition_1, \"\n              f\"{failed_condition_2.shape[0]} rows failed Condition_2.\")\n    else:\n        print(\"Data integrity check was successful! All rows pass the integrity conditions.\")\n        \n    return data\n\nprocessed_data = load_and_check()",
    "outputsMetadata": {
     "0": {
      "height": 101,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Data integrity check was successful! All rows pass the integrity conditions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ph/td7f8msd4md23pppxr8dfz_c0000gn/T/ipykernel_45330/316242998.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['Condition_1'].fillna(False, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_check():\n",
    "    # Step 1: Load the data and check if it has the expected shape\n",
    "    data = pd.read_csv('sales.csv')  \n",
    "    \n",
    "    expected_columns = 17\n",
    "    actual_columns = data.shape[1]\n",
    "    if actual_columns != expected_columns:\n",
    "        print(f\"Data column mismatch! Expected {expected_columns}, but got {actual_columns}.\")\n",
    "        print(f\"Columns found: {list(data.columns)}\")\n",
    "    else:\n",
    "        print(\"Data loaded successfully.\")\n",
    "\n",
    "    # Step 2: Calculate statistical values and merge with the original data\n",
    "    grouped_data = data.groupby(['Date'])['Total'].agg(['mean', 'std'])\n",
    "    grouped_data['threshold'] = 3 * grouped_data['std']\n",
    "    grouped_data['max'] = grouped_data['mean'] + grouped_data.threshold\n",
    "    grouped_data['min'] = grouped_data[['mean', 'threshold']].apply(lambda row: max(0, row['mean'] - row['threshold']), axis=1)\n",
    "    data = pd.merge(data, grouped_data, on='Date', how='left')\n",
    "\n",
    "    # Condition_1 checks if 'Total' is within the acceptable range (min to max) for each date\n",
    "    data['Condition_1'] = (data['Total'] >= data['min']) & (data['Total'] <= data['max'])\n",
    "    data['Condition_1'].fillna(False, inplace=True)\n",
    "\n",
    "    #Properly calculate the tax column\n",
    "    data['Tax'] = data['Quantity'] * data['Unit price'] * 0.05\n",
    "  \n",
    "\n",
    "    # Condition_2 checks if the 'Tax' column is properly calculated as 5% of (Quantity * Unit price)\n",
    "    data['Condition_2'] = round(data['Quantity'] * data['Unit price'] * 0.05, 1) == round(data['Tax'], 1)\n",
    "        \n",
    "    # Step 3: Check if all rows pass both Condition_1 and Condition_2\n",
    "    # Success indicates data integrity; failure suggests potential issues.\n",
    "    failed_condition_1 = data[~data['Condition_1']]\n",
    "    failed_condition_2 = data[~data['Condition_2']]\n",
    "\n",
    "    if failed_condition_1.shape[0] > 0 or failed_condition_2.shape[0] > 0:\n",
    "        print(f\"Data integrity check failed! {failed_condition_1.shape[0]} rows failed Condition_1, \"\n",
    "              f\"{failed_condition_2.shape[0]} rows failed Condition_2.\")\n",
    "    else:\n",
    "        print(\"Data integrity check was successful! All rows pass the integrity conditions.\")\n",
    "        \n",
    "    return data\n",
    "\n",
    "processed_data = load_and_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752b62a",
   "metadata": {},
   "source": [
    "#### What I did to fix the pipeline was to;\n",
    "\n",
    "Correctly assign expected_columns = 17, as i checked the actual columnss in the csv file and noticed it was 17 and not 18\n",
    "Properly calculate the tax column"
   ]
  }
 ],
 "metadata": {
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
